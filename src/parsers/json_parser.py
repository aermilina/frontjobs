import aiohttp
import logging
import os
from datetime import datetime
from typing import List, Tuple, Optional, Dict
from dateparser import parse
from src.utils.lastpublished import save_last_published_date
from src.utils.dateutils import to_utc, is_newer, update_last_published_date
from src.parsers.base_parser import VacancyParser
from src.utils.cleandescription import cleandescription


# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
logging.basicConfig(
    level=getattr(logging, log_level, logging.INFO),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class JSONParser(VacancyParser):
    def __init__(
        self,
        url: str,
        keywords: str,
        last_published_date: Optional[datetime],
        last_published_file: str
    ):
        super().__init__(last_published_date, last_published_file)
        self.api_url = url
        self.keywords = [kw.strip().lower() for kw in keywords.split(',')] if keywords else []

    async def fetch_vacancies(self) -> List[Tuple[str, str, Dict]]:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ Ğ¸Ğ· JSON API Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼ Ğ¸ Ğ´Ğ°Ñ‚Ğµ.
        """
        vacancies = []
        new_last_published_date = self.last_published_date

        if not self.api_url:
            logger.error("JSON_FEED Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ² .env")
            return []

        async with aiohttp.ClientSession() as session:
            logger.info(f"Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº JSON API: {self.api_url}")
            try:
                    async with session.get(self.api_url, ssl=False, timeout=10) as response:
                        response.raise_for_status()
                        data = await response.json()
            except aiohttp.ClientResponseError as e:
                    logger.error(f"HTTP-Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ API {self.api_url}: {e.status}, {e.message}")
                    return []
            except aiohttp.ClientError as e:
                    logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ API {self.api_url}: {e}")
                    if "SSL" in str(e):
                        logger.warning("SSL-Ğ¾ÑˆĞ¸Ğ±ĞºĞ°. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° SSL Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ°. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ñ‹.")
                    return []
            except ValueError as e:
                    logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ JSON Ğ¾Ñ‚ {self.api_url}: {e}")
                    return []

            if not isinstance(data, list):
                logger.error(f"ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… API: {type(data)}")
                return []

            for item in data:
                title = item.get('position', 'Ğ‘ĞµĞ· Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ')
                link = item.get('apply_url', '#')
                raw_description = item.get('description', '')
                pub_date_str = item.get('date', '')
                tags = [tag.lower() for tag in item.get('tags', []) if tag]
                company = item.get('company', 'None')
                salarymin = item.get('salary_min','None')
                salarymax = item.get ('salary_max','None')

                # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° HTML Ğ¸Ğ· Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ
                description = cleandescription(raw_description)
                salary = f"{str(salarymin)}$ - {str(salarymax)}$" if salarymin != '' or salarymax != '' else 'Not specified'


                # ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ°Ñ‚Ñ‹
                date_published = None
                if pub_date_str:
                    try:
                        parsed_date = parse(pub_date_str, settings={'TIMEZONE': 'UTC', 'TO_TIMEZONE': 'UTC'})
                        if parsed_date:
                            date_published = to_utc(parsed_date)
                            formatted_date = date_published.strftime('%d %B %Y')
                        else:
                            logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°Ğ·Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ñƒ: {pub_date_str}")
                    except Exception as e:
                        logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ°Ñ‚Ñ‹: {pub_date_str}, {e}")
                
                metadata = {
                    "description": description[:100] + "..." if len(description) > 100 else description,
                    "company": company,
                    "published_date": date_published.strftime('%Y-%m-%d %H:%M:%S'),
                    "published_date_str": formatted_date,
                    "salary": salary
                }

                # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ
                content = f"{title.lower()} {description.lower()} {' '.join(tags)}".strip()
                logger.debug(f"ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°: {self.keywords}, Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ: {content}")
                if date_published and is_newer(date_published, self.last_published_date) and \
                   (not self.keywords or any(keyword in content for keyword in self.keywords)):
                    vacancies.append((title, link, metadata))
                    new_last_published_date = update_last_published_date(new_last_published_date, date_published)
                    logger.info(f"Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ñ: {title}, {link}")

        if new_last_published_date:
            self.last_published_date = new_last_published_date
            save_last_published_date(new_last_published_date, self.last_published_file)

        return vacancies

    def format_message(self, title: str, link: str, metadata: Dict) -> str:
        """
        Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Telegram.
        """
        return (
            f"ğŸ“¡ **{title}**\n\n"
            f"ğŸ“… Published: {metadata['published_date_str']}\n\n"
            f"ğŸ¢ Company: {metadata['company']}\n\n"
            f"ğŸ“ Description: {metadata['description']}\n\n"
            f"ğŸ’µ Estimated salary: {metadata['salary']}\n\n"
            f"ğŸ‘‰[APPLY NOW]({link})"
        )