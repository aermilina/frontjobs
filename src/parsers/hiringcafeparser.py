import aiohttp
import logging
import os
from datetime import datetime
from typing import List, Tuple, Optional, Dict
from src.utils.lastpublished import save_last_published_date
from src.utils.dateutils import to_utc, is_newer, update_last_published_date
from src.parsers.base_parser import VacancyParser
from dateparser import parse
from src.utils.cleandescription import cleandescription


# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
logging.basicConfig(
    level=getattr(logging, log_level, logging.INFO),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class HiringCafeParser(VacancyParser):
    def __init__(
        self,
        url: str,
        last_published_date: Optional[datetime],
        last_published_file: str,
        keywords: str = "frontend"
    ):
        super().__init__(last_published_date, last_published_file)
        self.api_url = url
        self.keywords = keywords
        self.workplace_type = 'remote'

    async def fetch_vacancies(self) -> List[Tuple[str, str, Dict]]:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ Ğ¸Ğ· API Hiring Cafe Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğµ Ğ¸ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸Ğ· Ğ¿Ğ°Ğ³Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸.
        """
        vacancies = []
        new_last_published_date = self.last_published_date

        if not self.api_url:
            logger.error("HC_API_URL Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ² .env")
            return []

        payload = {
            "size": 20,
            "page": 0,
            "searchState": {
                "searchQuery": self.keywords,
                "sortBy": "date"
            }
        }

        async with aiohttp.ClientSession() as session:
            page = 0
            while True:
                payload["page"] = page
                logger.info(f"Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº API Hiring Cafe: {self.api_url}, ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {page}, payload={payload}")
                try:
                    async with session.post(self.api_url, json=payload, ssl=False, timeout=10) as response:
                        
                        content = await response.text()
                        if response.status == 401:
                            logger.error("ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (401): Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾ĞºĞµĞ½ API Hiring Cafe.")
                            return []
                        response.raise_for_status()
                        try:
                            data = await response.json()
                        except ValueError as e:
                            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ JSON: {e}, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ: {content}")
                            return []
                except aiohttp.ClientResponseError as e:
                    logger.error(f"HTTP-Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ API {self.api_url}: {e.status}, {e.message}")
                    return []
                except aiohttp.ClientError as e:
                    logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ API {self.api_url}: {e}")
                    if "SSL" in str(e):
                        logger.warning("SSL-Ğ¾ÑˆĞ¸Ğ±ĞºĞ°. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° SSL Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ°. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ñ‹.")
                    return []

                if not isinstance(data, dict) or 'results' not in data:
                    logger.error(f"ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… API: {type(data)}")
                    return []
                results = data.get('results', [])


                if not results:
                    logger.info(f"ĞĞµÑ‚ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ {page}")
                    break

                all_old = True  # Ğ¤Ğ»Ğ°Ğ³: Ğ²ÑĞµ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ ÑÑ‚Ğ°Ñ€Ñ‹Ğµ

                for item in results:
                    processed_data = item.get('v5_processed_job_data', {})
                    title = processed_data.get('core_job_title', 'Ğ‘ĞµĞ· Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ')
                    link = item.get('apply_url', '#')
                    workplace_type = processed_data.get('workplace_type', '').lower()
                    date_published_str = processed_data.get('estimated_publish_date', '')

                    # ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ°Ñ‚Ñ‹
                    date_published = None
                    if date_published_str:
                        try:
                            parsed_date = parse(date_published_str, settings={'TIMEZONE': 'UTC', 'TO_TIMEZONE': 'UTC'})
                            if parsed_date:
                                date_published = to_utc(parsed_date)
                                formatted_date = date_published.strftime('%d %B %Y')
                            else:
                                logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°Ğ·Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ñƒ: {date_published_str}")
                        except Exception as e:
                            logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ°Ñ‚Ñ‹: {date_published_str}, {e}")

                    description = processed_data.get('requirements_summary', '')
                    cleaned_description = cleandescription(description)
                    company = processed_data.get('company_name', 'Not specified')
                    salaryrange = processed_data.get('listed_compensation_frequency', 'ĞĞµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¾')
                    experience = processed_data.get('seniority_level', 'Not specified')
                    currency = processed_data.get('listed_compensation_currency', 'not specified')
                    languages = processed_data.get("language_requirements", [])
                    language_str = ", ".join(languages) if languages else "Not specified"

                    if salaryrange:
                        salarymin = processed_data.get(f'{salaryrange.lower()}_min_compensation') or ''
                        salarymax = processed_data.get(f'{salaryrange.lower()}_max_compensation') or ''
                        if salarymin and salarymax:
                            salary = f'{salarymin} {currency} - {salarymax} {currency} {salaryrange}'
                        elif salarymin:
                            salary = f'from {salarymin} {currency} {salaryrange}'
                        elif salarymax:
                            salary = f'to {salarymax} {currency} {salaryrange}'
                        else:
                            salary = 'Not specified'
                    else:
                        salary = 'Not specified'

                    metadata = {
                        "description": cleaned_description[:100] + "..." if len(cleaned_description) > 100 else cleaned_description,
                        "experience": experience,
                        "company": company,
                        "published_date": date_published.strftime('%Y-%m-%d %H:%M:%S') if date_published else "Not specified",
                        "published_date_str": formatted_date,
                        "salary": salary,
                        "language": language_str
                    }

                    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğµ Ğ¸ Ñ‚Ğ¸Ğ¿Ñƒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹
                    if date_published and is_newer(date_published, self.last_published_date) and 'remote' in workplace_type:
                        all_old = False
                        vacancies.append((title, link, metadata))
                        new_last_published_date = update_last_published_date(new_last_published_date, date_published)
                        logger.info(f"Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ñ: {title}, {link}")
                    else:
                        logger.debug(f"ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ° Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ñ: {title} (Ğ´Ğ°Ñ‚Ğ°: {date_published}, Ñ‚Ğ¸Ğ¿: {workplace_type})")

                if all_old:
                    logger.info(f"Ğ’ÑĞµ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ {page} ÑÑ‚Ğ°Ñ€Ñ‹Ğµ, Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½.")
                    break

                page += 1

        now = datetime.utcnow()
        self.last_published_date = now
        save_last_published_date(now, self.last_published_file)

        logger.info(f"Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹: {len(vacancies)}")
        return vacancies


    def format_message(self, title: str, link: str, metadata: Dict) -> str:
        return (
            f"ğŸ’¼ **{title}**\n\n"
            f"ğŸ“… Published: {metadata.get('published_date_str', 'Not specified')}\n\n"
            f"âŒ›ï¸ Experience: {metadata.get('experience', 'Not specified')}\n\n"
            f"ğŸ¢ Company: {metadata.get('company', 'Not specified')}\n\n"
            f"ğŸ“ Description: {metadata.get('description', '')}\n\n"
            f"ğŸ”¤ Language: {metadata.get('language', 'Not specified')}\n\n"
            f"ğŸ’µ Salary: {metadata.get('salary', 'Not specified')}\n\n"
            f"ğŸ‘‰[APPLY NOW]({link})"
        )