import aiohttp
import logging
import os
from datetime import datetime, timedelta
from typing import List, Tuple, Optional, Dict
from dateparser import parse
from src.utils.lastpublished import save_last_published_date, load_last_published_date
from src.utils.dateutils import to_utc, is_newer, update_last_published_date
from src.parsers.base_parser import VacancyParser
from src.utils.cleandescription import cleandescription
from src.utils.normalizetags import normalize_tag
from src.utils.escapehtml import escape_html
from src.utils.normalizetags import normalize_tags
from src.utils.getflags import get_flag_emoji




# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
logging.basicConfig(
    level=getattr(logging, log_level, logging.INFO),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class RapidParser(VacancyParser):
    def __init__(
        self,
        url: str,
        last_published_date: Optional[datetime],
        last_published_file: str,
        host: str = "",
        key: str = ""
    ):
        super().__init__(last_published_date, last_published_file)
        self.api_url = url
        self.host = host
        self.key = key

    async def fetch_vacancies(self) -> List[Tuple[str, str, Dict]]:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ Ğ¸Ğ· API Rapid Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğµ.
        """
        vacancies = []
        new_last_published_date = self.last_published_date

        if not self.api_url:
            logger.error("RAPID_API_URL Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½")
            return []

        # Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ´Ğ°Ñ‚Ñƒ Ğ·Ğ° Ğ²Ñ‡ĞµÑ€Ğ°
        yesterday = datetime.now() - timedelta(days=1)
        logger.info(f"Ğ”Ğ°Ñ‚Ğ° Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°: {yesterday}")
        logger.debug(f"ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ÑÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°: {self.last_published_date}")

        headers = {}
        if self.host and self.key:
            headers = {
                "x-rapidapi-key": self.key,
                "x-rapidapi-host": self.host
            }

        querystring = {
            "query": "frontend",
            "location": "any",
            "remoteOnly": "true",
            "employmentTypes": "fulltime;parttime;intern;contractor",
            "datePosted": "today"
        }

        async with aiohttp.ClientSession() as session:
            logger.info(f"Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº API Rapid: {self.api_url}, params={querystring}")
            try:
                async with session.request('GET', self.api_url, headers=headers, params=querystring, ssl=False, timeout=10) as response:
                    content = await response.text()
                    logger.debug(f"Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° API: {content}")
                    if response.status == 401:
                        logger.error("ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (401): Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ API Rapid.")
                        return []
                    response.raise_for_status()
                    try:
                        data = await response.json()
                    except ValueError as e:
                        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ JSON: {e}, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ: {content}")
                        return []
                    logger.debug(f"ĞÑ‚Ğ²ĞµÑ‚ API: {data}")
            except aiohttp.ClientResponseError as e:
                logger.error(f"HTTP-Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ API {self.api_url}: {e.status}, {e.message}")
                return []
            except aiohttp.ClientError as e:
                logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ API {self.api_url}: {e}")
                if "SSL" in str(e):
                    logger.warning("SSL-Ğ¾ÑˆĞ¸Ğ±ĞºĞ°. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° SSL Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ°. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ñ‹.")
                return []

            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
            if not isinstance(data, dict) or 'jobs' not in data:
                logger.error(f"ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… API: {type(data)}")
                return []
            results = data.get('jobs', [])
            logger.debug(f"ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¾ {len(results)} Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹")

            for item in results:
                title = item.get('title', 'Without title')
                logger.info(f"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸: {title}")
                providers = item.get('jobProviders', [])
                date_posted_str = item.get('datePosted', '')
                logger.info(f"date_posted_str: {date_posted_str}")

                # ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ°Ñ‚Ñ‹
                date_published = None
                if date_posted_str:
                    try:
                        parsed_date = parse(date_posted_str, settings={'TIMEZONE': 'UTC', 'TO_TIMEZONE': 'UTC'})
                        logger.info(f"Ğ”Ğ°Ñ‚Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸: {parsed_date}")
                        if parsed_date:
                            date_published = to_utc(parsed_date)
                            formatted_date = date_published.strftime('%d %B %Y')
                            logger.info(f"Ğ”Ğ°Ñ‚Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ (UTC): {date_published}")
                    except Exception as e:
                        logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ°Ñ‚Ñ‹: {date_posted_str}, {e}")

                # ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°Ğ¼Ğ¸)
                description = item.get('description', '')
                cleaned_description = cleandescription(description)
                company = item.get('company', 'Not specified')
                salaryrange = item.get('salaryRange', 'Not specified')
                location = item.get('location', 'Not specified')
                flag = get_flag_emoji(location)
                location_tag = normalize_tag(location)
                hashtags = []
                if location.lower() != "not specified" and location_tag:
                    hashtags.append(location_tag)

                if salaryrange:
                    salary = salaryrange
                else:
                    salary = f'Not specified'

                metadata = {
                    "description": cleaned_description[:100] + "..." if len(cleaned_description) > 100 else cleaned_description,
                    "company": company,
                    "published_date": date_published.strftime('%Y-%m-%d %H:%M:%S') if date_published else "Not specified",
                    "published_date_str": formatted_date,
                    "salary": salary,
                    "location": location,
                    "flag": flag or "",
                    "hashtags": hashtags
                }

                # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğµ
                if date_published and is_newer(date_published, self.last_published_date):
                    link = providers[0].get('url', '#') if providers else 'No link'
                    vacancies.append((title, link, metadata))
                    logger.info(f"Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ñ: {title}, {link}")
                    new_last_published_date = update_last_published_date(new_last_published_date, date_published)
                else:
                    logger.debug(f"Ğ’Ğ°ĞºĞ°Ğ½ÑĞ¸Ñ '{title}' Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ: date_published={date_published}, last_published_date={self.last_published_date}")

        if new_last_published_date:
            self.last_published_date = new_last_published_date
            save_last_published_date(new_last_published_date, self.last_published_file)

        logger.info(f"Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹: {len(vacancies)}")
        return vacancies

    def format_message(self, title: str, link: str, metadata: Dict) -> str:
        hashtags = metadata.get("hashtags", [])
        hashtags_str = " ".join(escape_html(tag) for tag in hashtags if tag)

        title_escaped = escape_html(title)
        description = escape_html(metadata.get("description", ""))
        company = escape_html(metadata.get("company", "Not specified"))
        raw_location = metadata.get("location", "").strip()
        location = escape_html(raw_location) if raw_location else "Not specified"
        flag = metadata.get("flag", "")
        salary = escape_html(metadata.get("salary", "Not specified"))
        published_date_str = escape_html(metadata.get("published_date_str", "Not specified"))

        return (
            f"ğŸ’¼ <b>{title_escaped}</b>\n"
            f"ğŸ“ Location: {flag} {location}\n\n"
            f"ğŸ“… Published: {published_date_str}\n\n"
            f"ğŸ¢ Company: {company}\n"
            f"ğŸ“ Description: {description}\n\n"
            f"ğŸ’µ Salary: {salary}\n\n"
            f"ğŸ‘‰ <a href=\"{link}\">APPLY NOW</a>\n\n"
            f"{hashtags_str}"
        )